{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35da04e-fb88-4e89-a4a1-95845622f244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0cd970c-9c90-4306-83d0-d91a2b53bd99",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "# Process Features table to Silver with SCD Type 2 (FIXED - Works in ONE run)\n",
    "\n",
    "# Step 1: Read from bronze and fix data types\n",
    "df_features = spark.table(\"practise.bronze.features\")\n",
    "for c in [\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"CPI\", \"Unemployment\"]:\n",
    "    df_features = df_features.withColumn(c, expr(f\"try_cast({c} as double)\"))\n",
    "\n",
    "# Step 2: Add SCD Type 2 tracking columns\n",
    "df_features = df_features.drop(\"Bronze_Ing_Time\")\n",
    "df_features = df_features.withColumn(\"Silver_Ing_Time\", current_timestamp())\n",
    "df_features = df_features.withColumn(\"is_current\", lit(True))\n",
    "df_features = df_features.withColumn(\"effective_start_date\", current_timestamp())\n",
    "df_features = df_features.withColumn(\"effective_end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "# Step 3: Merge to silver table\n",
    "if spark.catalog.tableExists(\"practise.silver.features\"):\n",
    "    # FIXED: First expire old records, then insert new ones\n",
    "    \n",
    "    # Part 1: Expire old records that changed\n",
    "    DeltaTable.forName(spark, \"practise.silver.features\").alias(\"old\").merge(\n",
    "        df_features.alias(\"new\"),\n",
    "        \"old.Store = new.Store AND old.Date = new.Date AND old.is_current = true\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"old.Temperature != new.Temperature OR old.Fuel_Price != new.Fuel_Price OR old.MarkDown1 != new.MarkDown1 OR old.MarkDown2 != new.MarkDown2 OR old.MarkDown3 != new.MarkDown3 OR old.MarkDown4 != new.MarkDown4 OR old.MarkDown5 != new.MarkDown5 OR old.CPI != new.CPI OR old.Unemployment != new.Unemployment OR old.IsHoliday != new.IsHoliday\",\n",
    "        set={\"is_current\": \"false\", \"effective_end_date\": \"current_timestamp()\"}\n",
    "    ).execute()\n",
    "    \n",
    "    # Part 2: Insert new/changed records\n",
    "    DeltaTable.forName(spark, \"practise.silver.features\").alias(\"old\").merge(\n",
    "        df_features.alias(\"new\"),\n",
    "        \"old.Store = new.Store AND old.Date = new.Date AND old.is_current = true\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "    \n",
    "    print(\"✅ Merged to practise.silver.features (in ONE run)\")\n",
    "else:\n",
    "    # First time: Create the table\n",
    "    df_features.write.format(\"delta\").saveAsTable(\"practise.silver.features\")\n",
    "    print(\"✅ Created practise.silver.features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c19c296-f8ed-44f1-9991-d87c8c524b76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [],
   "source": [
    "# Process Sales table to Silver with SCD Type 2 (FIXED - Works in ONE run)\n",
    "\n",
    "# Step 1: Read from bronze\n",
    "df_sales = spark.table(\"practise.bronze.sales\")\n",
    "\n",
    "# Step 2: Add SCD Type 2 tracking columns\n",
    "df_sales = df_sales.drop(\"Bronze_Ing_Time\")\n",
    "df_sales = df_sales.withColumn(\"Silver_Ing_Time\", current_timestamp())\n",
    "df_sales = df_sales.withColumn(\"is_current\", lit(True))\n",
    "df_sales = df_sales.withColumn(\"effective_start_date\", current_timestamp())\n",
    "df_sales = df_sales.withColumn(\"effective_end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "# Step 3: Merge to silver table\n",
    "if spark.catalog.tableExists(\"practise.silver.sales\"):\n",
    "    # FIXED: First expire old records, then insert new ones\n",
    "    \n",
    "    # Part 1: Expire old records that changed\n",
    "    DeltaTable.forName(spark, \"practise.silver.sales\").alias(\"old\").merge(\n",
    "        df_sales.alias(\"new\"),\n",
    "        \"old.Store = new.Store AND old.Dept = new.Dept AND old.Date = new.Date AND old.is_current = true\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"old.Weekly_Sales != new.Weekly_Sales OR old.IsHoliday != new.IsHoliday\",\n",
    "        set={\"is_current\": \"false\", \"effective_end_date\": \"current_timestamp()\"}\n",
    "    ).execute()\n",
    "    \n",
    "    # Part 2: Insert new/changed records\n",
    "    DeltaTable.forName(spark, \"practise.silver.sales\").alias(\"old\").merge(\n",
    "        df_sales.alias(\"new\"),\n",
    "        \"old.Store = new.Store AND old.Dept = new.Dept AND old.Date = new.Date AND old.is_current = true\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "    \n",
    "    print(\"✅ Merged to practise.silver.sales (in ONE run)\")\n",
    "else:\n",
    "    # First time: Create the table\n",
    "    df_sales.write.format(\"delta\").saveAsTable(\"practise.silver.sales\")\n",
    "    print(\"✅ Created practise.silver.sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe74adf0-c808-490f-90e9-24e41837c104",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "# Process Stores table to Silver with SCD Type 2 (FIXED - Works in ONE run)\n",
    "\n",
    "# Step 1: Read from bronze\n",
    "df_stores = spark.table(\"practise.bronze.stores\")\n",
    "\n",
    "# Step 2: Add SCD Type 2 tracking columns\n",
    "df_stores = df_stores.drop(\"Bronze_Ing_Time\")\n",
    "df_stores = df_stores.withColumn(\"Silver_Ing_Time\", current_timestamp())\n",
    "df_stores = df_stores.withColumn(\"is_current\", lit(True))\n",
    "df_stores = df_stores.withColumn(\"effective_start_date\", current_timestamp())\n",
    "df_stores = df_stores.withColumn(\"effective_end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "# Step 3: Merge to silver table\n",
    "if spark.catalog.tableExists(\"practise.silver.stores\"):\n",
    "    # FIXED: First expire old records, then insert new ones\n",
    "    \n",
    "    # Part 1: Expire old records that changed\n",
    "    DeltaTable.forName(spark, \"practise.silver.stores\").alias(\"old\").merge(\n",
    "        df_stores.alias(\"new\"),\n",
    "        \"old.Store = new.Store AND old.is_current = true\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"old.Type != new.Type OR old.Size != new.Size\",\n",
    "        set={\"is_current\": \"false\", \"effective_end_date\": \"current_timestamp()\"}\n",
    "    ).execute()\n",
    "    \n",
    "    # Part 2: Insert new/changed records (records that don't have a current version)\n",
    "    DeltaTable.forName(spark, \"practise.silver.stores\").alias(\"old\").merge(\n",
    "        df_stores.alias(\"new\"),\n",
    "        \"old.Store = new.Store AND old.is_current = true\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "    \n",
    "    print(\"✅ Merged to practise.silver.stores (in ONE run)\")\n",
    "else:\n",
    "    # First time: Create the table\n",
    "    df_stores.write.format(\"delta\").saveAsTable(\"practise.silver.stores\")\n",
    "    print(\"✅ Created practise.silver.stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b58dc2e8-cb71-46ee-84da-1bcb13c1e694",
     "showTitle": true,
     "tableResultSettingsMap": {
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770665438438}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      }
     },
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "# Verify silver tables with row counts\n",
    "for tbl in [\"features\", \"sales\", \"stores\"]:\n",
    "    t = spark.table(f\"practise.silver.{tbl}\")\n",
    "    print(f\"\\n=== SILVER.{tbl.upper()} ===\")\n",
    "    print(f\"Total: {t.count():,} | Current: {t.filter(col('is_current') == True).count():,}\")\n",
    "    display(t.filter(col(\"is_current\") == True).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4bcae21-a014-45c7-bb46-828a3a37358e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770665531683}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"practise.silver.stores\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "To_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
